{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `parser.ipynb` **v01** \n",
    "\n",
    "### interactively developing the `parser.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install html5lib bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime, csv\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup # pip3 install html5lib bs4 \n",
    "\n",
    "from settings import downloadsFolder, platformsOrdered, COLUMN_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply read all files that begin with pc_, switch_, playstation-4_\n",
    "filenames = sorted([name for name in os.listdir(downloadsFolder) \n",
    "                     if name.split(\"_\")[0] in platformsOrdered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseMetascore(soup, urlpath, resultsDict):\n",
    "    \"\"\"\n",
    "    find the 'metascore' data in the page, by finding the relevant HTML tags\n",
    "    \"\"\"\n",
    "    # metascore\n",
    "    ms = soup.find('div', attrs = {'class':'score_summary metascore_summary'})\n",
    "    metascoreFind = ms.find('span', attrs = {'itemprop':'ratingValue'})\n",
    "    resultsDict[\"metascore\"] = int(metascoreFind.text) if metascoreFind else 0\n",
    "\n",
    "    # metascore number of reviews\n",
    "    summary = ms.find('div', attrs = {'class' : 'summary'})\n",
    "    criticReviews = summary.find('a', attrs = {'href':'%s/critic-reviews' % urlpath})\n",
    "    resultsDict[\"metascoreBased\"] = int(criticReviews.find('span').text.strip()) if criticReviews else 0\n",
    "\n",
    "\n",
    "def parseUserscore(soup, urlpath, resultsDict):\n",
    "    \"\"\"\n",
    "    find the 'userscore' data in the page, by finding the relevant HTML tags\n",
    "    \"\"\"\n",
    "    # userscore\n",
    "    us = soup.find('div', attrs = {'class':'userscore_wrap feature_userscore'})\n",
    "    # print (us.prettify())\n",
    "    userscoreTags = us.select(\"div[class^=metascore_w\\ user\\ large\\ game]\") # begins with operator\n",
    "    if len(userscoreTags) !=1: # protect against a case that shouldn't happen anyway \n",
    "        raise Error(\"number of userscore tags not equal 1\")\n",
    "    userscoreText = userscoreTags[0].text.strip()\n",
    "    resultsDict[\"userscore\"] = 0 if userscoreText==\"tbd\" else float(userscoreText)\n",
    "\n",
    "    # userscore number of reviews\n",
    "    usersummary = us.find('div', attrs = {'class' : 'summary'})\n",
    "    userReviews = usersummary.find('a', attrs = {'href':'%s/user-reviews' % urlpath})\n",
    "    answer=0\n",
    "    if userReviews:\n",
    "        answer = int(userReviews.text.replace(\"Ratings\",\"\").strip())\n",
    "    else:\n",
    "        # if there aren't enough ratings yet, \n",
    "        # they don't tell us how many there are, but how many are still missing\n",
    "        um = usersummary.find('span', attrs = {'class':'connect4_msg'}).text.strip()\n",
    "        answer = -int(um.replace(\"Awaiting\",\"\").replace(\"more rating\",\"\").replace(\"s\",\"\"))\n",
    "    resultsDict[\"userscoreBased\"] = answer\n",
    "\n",
    "\n",
    "def parseOtherInfos(soup, resultsDict):\n",
    "    \"\"\"\n",
    "    find other info in the page, by searching within the text body\n",
    "    \"\"\"\n",
    "    # forget HTML, just parse the text\n",
    "    textlines = [lines.strip() for lines in soup.body.text.split(\"\\n\") \n",
    "                    if lines.strip() != \"\"]\n",
    "    #print(\"\\n\".join(textlines))\n",
    "    \n",
    "    # number of players\n",
    "    try:\n",
    "        nopsIndex = textlines.index(\"# of players:\")\n",
    "        nops = textlines[nopsIndex+1] if nopsIndex else \"\"\n",
    "    except:\n",
    "        nops = \"\"\n",
    "    resultsDict[\"nops\"]=nops\n",
    "\n",
    "    # developer company & release date\n",
    "    resultsDict[\"developer\"] = textlines[textlines.index(\"Developer:\")+1]\n",
    "    resultsDict[\"released\"] = textlines[textlines.index(\"Release Date:\")+1]\n",
    "\n",
    "    # genres are all in one line, but with many spaces inbetween    \n",
    "    i = next(i for i,text in enumerate(textlines) if text.startswith(\"Genre(s):\"))\n",
    "    resultsDict[\"genres\"] = textlines[i].replace(\"Genre(s):\", \"\").replace(\" \", \"\") # .split(\",\")\n",
    "\n",
    "    i=textlines.index(\"Publisher:\")\n",
    "    j=textlines.index(\"Release Date:\")\n",
    "    resultsDict[\"publisher\"] = \"\".join([line.strip() for line in textlines[i+1:j]])\n",
    "    # print(i, j, resultsDict[\"publisher\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseMetacriticFiles(filenames, downloadsFolder):\n",
    "    \"\"\"\n",
    "    read all files, parse content on HTML tag level, and on text level\n",
    "    \"\"\"\n",
    "    filename2results={}\n",
    "    for i, name in enumerate(filenames): # [33:34]):\n",
    "        platform, rest = name.split(\"_\")\n",
    "        game = rest.replace(\".html\", \"\")\n",
    "        resultsDict={\"platform\" : platform, \"game\": game}\n",
    "        urlpath = \"/game/%s/%s\" % (platform, game)\n",
    "        print (i, platform, game, end =\": \") # urlpath, end=\" \")\n",
    "\n",
    "        # read page file and turn into tag soup \n",
    "        with open(os.path.join(downloadsFolder, name), \"r\") as f:\n",
    "            page = f.read()\n",
    "        soup = BeautifulSoup(page, 'html5lib')\n",
    "        # print(soup.prettify())\n",
    "\n",
    "        # metascore\n",
    "        parseMetascore(soup, urlpath, resultsDict)\n",
    "        print (\"ms={metascore:d} ({metascoreBased:d} revs)\".format(**resultsDict), end=\"\")\n",
    "\n",
    "        # userscore\n",
    "        resultsDict[\"userscore\"], resultsDict[\"userscoreBased\"] = 0, 0\n",
    "        try: # there are faulty pages, with (tm) in the page URL, just ignore:\n",
    "            parseUserscore(soup, urlpath, resultsDict)\n",
    "        except:\n",
    "            pass\n",
    "        print (\"; us={userscore:.1f} ({userscoreBased:d} revs)\".format(**resultsDict), end=\"\")\n",
    "\n",
    "        # various other infos\n",
    "        parseOtherInfos(soup, resultsDict)\n",
    "        mystring=\"; released={released:s}; Dev={developer:s}; Publ={publisher:s}; Genres={genres:s}; #plyrs={nops:s}\"\n",
    "        print (mystring.format(**resultsDict))\n",
    "\n",
    "        # append to results dict\n",
    "        filename2results[game+\"_\"+platform] = resultsDict\n",
    "\n",
    "    return filename2results\n",
    "\n",
    "filename2results = parseMetacriticFiles(filenames, downloadsFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(filename2results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileGenres(filename2results):\n",
    "    \"\"\"\n",
    "    Had hoped to sort by single genre, but ~75 genres are a bit much.\n",
    "    Idea for a TODO: Group these 75 genres into 7-10 genregroups, then\n",
    "    create new table with 7-10 genregroups as titles, and Y/N columns.\n",
    "    \"\"\"\n",
    "    allGenres=[]\n",
    "    for fn,r in filename2results.items():\n",
    "        genres = r[\"genres\"].split(\",\")\n",
    "        allGenres.extend(genres)\n",
    "        # if \"\" in genres: print(fn) # these have unnecessary , in Genre(s)\n",
    "    allGenres = sorted(list(set(allGenres))) # make unique\n",
    "    if \"\" in allGenres:\n",
    "        allGenres.remove(\"\") # remove the empty genre\n",
    "    return allGenres\n",
    "\n",
    "allGenres = compileGenres(filename2results)\n",
    "print(\"#genres=%d:\\n%s\" % (len(allGenres), allGenres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResults(filename2results, filename=\"MyEpicGamesOnMetacritic-%s.csv\",\n",
    "                columnOrder=COLUMN_ORDER, folder=downloadsFolder):\n",
    "    \"\"\"\n",
    "    results into a timestamped csv file, and genres into a txt file\n",
    "    \"\"\"\n",
    "    timestamp=datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    fn=os.path.join(folder, filename % timestamp)\n",
    "    with open(fn,\"w\", newline='') as f:\n",
    "        csvwriter = csv.writer(f, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csvwriter.writerow(columnOrder)\n",
    "        for res in filename2results.values():\n",
    "            row=[res[c] for c in columnOrder]\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "    allGenres = compileGenres(filename2results)\n",
    "    fn2 = fn.replace(\".csv\", \"_genres.txt\")\n",
    "    with open(fn2,\"w\") as f:\n",
    "        for genre in allGenres:\n",
    "            f.write(genre+\"\\n\")\n",
    "\n",
    "saveResults(filename2results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc82e8d994478d839412480effdb8b58988141640f3c58839d054a90b7e382bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
